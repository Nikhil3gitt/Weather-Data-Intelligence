{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f06a5b2-c196-405a-811d-9652ab45b002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The purpose of this code is to ensure that the Azure Data Lake Storage directory for Open Weather API data is properly mounted. It first checks if the specified mount point is already active to avoid redundancy. If not, it securely mounts the directory using OAuth credentials. This step is crucial for establishing seamless access to cloud storage, enabling efficient data ingestion and processing.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b43fb3-8363-4a86-adae-6eaf3396b15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/preprocessdata/Open-weather-API/ is already mounted.\n"
     ]
    }
   ],
   "source": [
    "mount_point = \"/mnt/preprocessdata/Open-weather-API/\"\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    print(f\"{mount_point} is already mounted.\")\n",
    "else:\n",
    "    # Configuration for Azure Data Lake Storage with OAuth credentials\n",
    "    configs = {\n",
    "        \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id\": \"56105415-cb9f-436c-b2b6-a0472cf7aecb\",\n",
    "        \"fs.azure.account.oauth2.client.secret\": \"d568Q~ib_Do2M6XP9RuCK8hqBFMQc.HtyF4CObo7\",\n",
    "        \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/580abb99-e0ac-43be-805d-b1551dffaa63/oauth2/token\"\n",
    "    }\n",
    "\n",
    "    # Mount the directory if not already mounted\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://mymaincontainer@mainprojectwis.dfs.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=configs\n",
    "    )\n",
    "    print(f\"Mounted {mount_point} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0479dd-f1c7-4e46-933f-378f81d2d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/preprocessdata/Open-weather-API/Open-weather-API/cleaned-data/</td><td>cleaned-data/</td><td>0</td><td>1733091520000</td></tr><tr><td>dbfs:/mnt/preprocessdata/Open-weather-API/Open-weather-API/raw-data.json</td><td>raw-data.json</td><td>38003</td><td>1733010173000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/preprocessdata/Open-weather-API/Open-weather-API/cleaned-data/",
         "cleaned-data/",
         0,
         1733091520000
        ],
        [
         "dbfs:/mnt/preprocessdata/Open-weather-API/Open-weather-API/raw-data.json",
         "raw-data.json",
         38003,
         1733010173000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/preprocessdata/Open-weather-API/Open-weather-API\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac3ccdc-f26f-4f79-a8b8-0c36a05fa2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's process a raw JSON dataset containing weather data by flattening its nested structure and transforming key attributes, such as temperature, into human-readable formats (e.g., Celsius). It also extracts relevant attributes like humidity, wind speed, and weather descriptions for analysis and saves the cleaned dataset as a CSV file for further use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d099ba-6c2c-47f7-b318-497ab7ff8ab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Load the JSON file\n",
    "raw_data = spark.read.format(\"json\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .load(\"/mnt/preprocessdata/Open-weather-API/Open-weather-API/raw-data.json\")\n",
    "\n",
    "# Flatten the nested structure and convert temperature to Celsius\n",
    "cleaned_data = raw_data.select(\n",
    "    explode(col(\"list\")).alias(\"list_flat\")\n",
    ").select(\n",
    "    col(\"list_flat.dt\").alias(\"timestamp\"),\n",
    "    ((col(\"list_flat.main.temp\") - 273.15)).alias(\"temperature_celsius\"),  # Convert to Celsius\n",
    "    ((col(\"list_flat.main.feels_like\") - 273.15)).alias(\"feels_like_celsius\"),  # Convert to Celsius\n",
    "    col(\"list_flat.main.pressure\").alias(\"pressure\"),\n",
    "    col(\"list_flat.main.humidity\").alias(\"humidity\"),\n",
    "    col(\"list_flat.main.temp_min\").alias(\"temp_min_celsius\"),\n",
    "    col(\"list_flat.main.temp_max\").alias(\"temp_max_celsius\"),\n",
    "    col(\"list_flat.wind.speed\").alias(\"wind_speed\"),\n",
    "    col(\"list_flat.wind.deg\").alias(\"wind_direction\"),\n",
    "    col(\"list_flat.clouds.all\").alias(\"clouds\"),\n",
    "    explode(col(\"list_flat.weather\")).alias(\"weather_flat\")\n",
    ").select(\n",
    "    col(\"timestamp\"),\n",
    "    col(\"temperature_celsius\"),\n",
    "    col(\"feels_like_celsius\"),\n",
    "    col(\"pressure\"),\n",
    "    col(\"humidity\"),\n",
    "    col(\"temp_min_celsius\"),\n",
    "    col(\"temp_max_celsius\"),\n",
    "    col(\"wind_speed\"),\n",
    "    col(\"wind_direction\"),\n",
    "    col(\"clouds\"),\n",
    "    col(\"weather_flat.main\").alias(\"weather_main\"),\n",
    "    col(\"weather_flat.description\").alias(\"weather_description\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2845d73c-41e7-4b7a-bdf5-4b65c3a41a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame if needed\n",
    "if not isinstance(cleaned_data, pd.DataFrame):\n",
    "    cleaned_data = cleaned_data.toPandas()\n",
    "\n",
    "# Map existing column names to the desired column names\n",
    "column_mapping = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"temperature_celsius\": \"temperature\",\n",
    "    \"feels_like_celsius\": \"dew_point_temperature\",  # Assuming alignment with \"feels_like_celsius\"\n",
    "    \"pressure\": \"station_level_pressure\",\n",
    "    \"humidity\": \"relative_humidity\",\n",
    "    \"temp_min_celsius\": \"temp_min_celsius\",  # Placeholder for non-target columns\n",
    "    \"temp_max_celsius\": \"temp_max_celsius\",  # Placeholder for non-target columns\n",
    "    \"wind_speed\": \"wind_speed\",\n",
    "    \"wind_direction\": \"wind_direction\",\n",
    "    \"clouds\": \"Sky_Cover\",\n",
    "    \"weather_main\": \"Weather_Description\",\n",
    "    \"weather_description\": \"Weather_Description\"  # Assuming redundancy\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "data = cleaned_data.rename(columns=column_mapping)\n",
    "\n",
    "# Define target schema and retain matching columns\n",
    "target_columns = [\n",
    "    \"temperature\", \"dew_point_temperature\", \"station_level_pressure\",\n",
    "    \"sea_level_pressure\", \"wind_direction\", \"wind_speed\", \"precipitation\",\n",
    "    \"relative_humidity\", \"visibility\", \"altimeter\", \"timestamp\",\n",
    "    \"Weather_Description\", \"Sky_Cover\"\n",
    "]\n",
    "data = data[[col for col in target_columns if col in data.columns]]\n",
    "\n",
    "# Define the output directory and file path\n",
    "output_directory = \"/dbfs/mnt/preprocessdata/Open-weather-API/Open-weather-API/cleaned-data\"\n",
    "output_csv_path = os.path.join(output_directory, \"cleaned_data.csv\")\n",
    "\n",
    "# Create the directory if it doesn't exist and save the DataFrame to a CSV file\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "data.to_csv(output_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2571236360262242,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Api-data transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}